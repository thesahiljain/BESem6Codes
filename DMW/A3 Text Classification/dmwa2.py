# -*- coding: utf-8 -*-
"""DMWA2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJJ--Vhq0YPt96woW488TsfcDZmFhVQN
"""

import nltk
nltk.download('movie_reviews')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import movie_reviews
from nltk.corpus import stopwords

import string
from nltk.stem import PorterStemmer, WordNetLemmatizer
ps = PorterStemmer()
lm = WordNetLemmatizer()
sw = stopwords.words('english')
def preprocesslist(words):
  new = list()
  for w in list(words):
    if w in sw or w in string.punctuation:
      continue
    w = lm.lemmatize(ps.stem(w))
    new.append(w)
  return new

print('Stopwords : ', sw)
print('Punctuation : ', string.punctuation)

documents = [(preprocesslist(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]

import random
random.shuffle(documents)
print(documents[0])
import string
print(',' in string.punctuation)

all_words = nltk.FreqDist([w.lower() for w in preprocesslist(movie_reviews.words())])
print(all_words.most_common(15))

word_features = list(dict(all_words.most_common(3000)).keys())

def find_features(document):
  words = set(document)
  return dict((w, w in words) for w in word_features)

featuresets = [(find_features(rev), category) for (rev, category) in documents]

print(featuresets[100])

print(len(featuresets))
train_set = featuresets[:1900]
test_set = featuresets[1900:]

clf = nltk.NaiveBayesClassifier.train(train_set)

print(nltk.classify.accuracy(clf, test_set)*100,'%')

test_input = list()
test_output = list()
for input, output in test_set:
  test_input.append(input)
  test_output.append(output)
predictions = clf.classify_many(test_input)

from sklearn.metrics import classification_report
print(classification_report(test_output, predictions))